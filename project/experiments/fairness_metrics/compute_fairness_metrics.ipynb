{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =====================================================\n",
    "# Synthetic example (matching your Word documentation)\n",
    "# =====================================================\n",
    "\n",
    "# 4 חתולים (קבוצת מיעוט), 102 כלבים (קבוצת רוב)\n",
    "# המודל \"מוטה\": מסווג הכל כ\"כלב\"\n",
    "y_true = np.array([0] * 4 + [1] * 102)  # 0 = Cat, 1 = Dog\n",
    "y_pred = np.array([1] * 106)  # המודל מסווג תמיד \"כלב\"\n",
    "\n",
    "# ניצור גם הסתברויות סינתטיות\n",
    "# חתולים: המודל \"בטוח\" שהם כלבים\n",
    "# כלבים: המודל נותן הסתברות גבוהה\n",
    "y_prob = np.concatenate(\n",
    "    [\n",
    "        np.random.uniform(0.6, 0.9, size=4),  # חתולים - תחזית שגויה ובטוחה\n",
    "        np.random.uniform(0.8, 1.0, size=102),  # כלבים\n",
    "    ]\n",
    ")\n",
    "\n",
    "group_cats = y_true == 0\n",
    "group_dogs = y_true == 1\n",
    "\n",
    "# =====================================================\n",
    "# Helper functions (Fairness Metrics)\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "def accuracy(true, pred, mask=None):\n",
    "    if mask is None:\n",
    "        return np.mean(true == pred)\n",
    "    return np.mean(true[mask] == pred[mask])\n",
    "\n",
    "\n",
    "def demographic_parity(pred, mask):\n",
    "    \"\"\"P(ŷ = 1 | group)\"\"\"\n",
    "    return np.mean(pred[mask] == 1)\n",
    "\n",
    "\n",
    "def tpr(true, pred, mask):\n",
    "    \"\"\"True Positive Rate\"\"\"\n",
    "    tp = np.sum((true[mask] == 1) & (pred[mask] == 1))\n",
    "    p = np.sum(true[mask] == 1)\n",
    "    return tp / p if p > 0 else 0\n",
    "\n",
    "\n",
    "def fpr(true, pred, mask):\n",
    "    \"\"\"False Positive Rate\"\"\"\n",
    "    fp = np.sum((true[mask] == 0) & (pred[mask] == 1))\n",
    "    n = np.sum(true[mask] == 0)\n",
    "    return fp / n if n > 0 else 0\n",
    "\n",
    "\n",
    "def disparate_impact(dp_minority, dp_majority):\n",
    "    \"\"\"DI = P(ŷ=1 | minority) / P(ŷ=1 | majority)\"\"\"\n",
    "    return dp_minority / dp_majority if dp_majority > 0 else 0\n",
    "\n",
    "\n",
    "def calibration_mean(prob, true, mask):\n",
    "    \"\"\"mean predicted probability + mean actual correctness\"\"\"\n",
    "    return np.mean(prob[mask]), np.mean((prob[mask] > 0.5) == true[mask])\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# COMPUTE METRICS\n",
    "# =====================================================\n",
    "\n",
    "overall_acc = accuracy(y_true, y_pred)\n",
    "acc_cats = accuracy(y_true, y_pred, group_cats)\n",
    "acc_dogs = accuracy(y_true, y_pred, group_dogs)\n",
    "\n",
    "dp_cats = demographic_parity(y_pred, group_cats)\n",
    "dp_dogs = demographic_parity(y_pred, group_dogs)\n",
    "\n",
    "tpr_cats = tpr(y_true, y_pred, group_cats)\n",
    "tpr_dogs = tpr(y_true, y_pred, group_dogs)\n",
    "\n",
    "fpr_cats = fpr(y_true, y_pred, group_cats)\n",
    "fpr_dogs = fpr(y_true, y_pred, group_dogs)\n",
    "\n",
    "di_score = disparate_impact(dp_cats, dp_dogs)\n",
    "\n",
    "calib_cats = calibration_mean(y_prob, y_true, group_cats)\n",
    "calib_dogs = calibration_mean(y_prob, y_true, group_dogs)\n",
    "\n",
    "print(\"=== Fairness Metrics (Synthetic Example) ===\")\n",
    "print(f\"Overall Accuracy: {overall_acc:.3f}\")\n",
    "print(f\"Accuracy  - Cats: {acc_cats:.3f}, Dogs: {acc_dogs:.3f}\")\n",
    "print(f\"DP        - Cats: {dp_cats:.3f}, Dogs: {dp_dogs:.3f}\")\n",
    "print(f\"TPR       - Cats: {tpr_cats:.3f}, Dogs: {tpr_dogs:.3f}\")\n",
    "print(f\"FPR       - Cats: {fpr_cats:.3f}, Dogs: {fpr_dogs:.3f}\")\n",
    "print(f\"Disparate Impact: {di_score:.3f}\")\n",
    "print(f\"Calibration (mean prob) - Cats: {calib_cats[0]:.3f}, Dogs: {calib_dogs[0]:.3f}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# BAR PLOT – Metrics per group\n",
    "# =====================================================\n",
    "\n",
    "metrics_labels = [\n",
    "    \"Accuracy\",\n",
    "    \"Demographic Parity\",\n",
    "    \"TPR (Equal Opportunity)\",\n",
    "    \"FPR (Equalized Odds)\",\n",
    "    \"Calibration (mean prob)\",\n",
    "]\n",
    "\n",
    "cats_values = [acc_cats, dp_cats, tpr_cats, fpr_cats, calib_cats[0]]\n",
    "\n",
    "dogs_values = [acc_dogs, dp_dogs, tpr_dogs, fpr_dogs, calib_dogs[0]]\n",
    "\n",
    "x = np.arange(len(metrics_labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(x - width / 2, cats_values, width, label=\"Cats (Minority)\")\n",
    "plt.bar(x + width / 2, dogs_values, width, label=\"Dogs (Majority)\")\n",
    "\n",
    "plt.xticks(x, metrics_labels, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Fairness Metrics Comparison (Synthetic Example)\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
